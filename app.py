"""
AI æ–‡ç« æª¢æ¸¬å™¨ Streamlit æ‡‰ç”¨ç¨‹å¼
"""

import streamlit as st
import time
import numpy as np
from typing import Dict
import re
from collections import Counter

# ==================== AI æª¢æ¸¬æ¨¡å‹ ====================

class SimpleAIDetector:
    """
    ç°¡åŒ–ç‰ˆçš„ AI æª¢æ¸¬å™¨ï¼Œä½¿ç”¨åŸºæœ¬çš„æ–‡å­—ç‰¹å¾µåˆ†æ
    çµåˆå¤šç¨®å•Ÿç™¼å¼è¦å‰‡ä¾†æé«˜æº–ç¢ºåº¦
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æª¢æ¸¬å™¨"""
        # AI å¸¸ç”¨çš„é€£æ¥è©å’Œè½‰æŠ˜è©
        self.ai_markers = {
            'however', 'moreover', 'furthermore', 'additionally', 'consequently',
            'therefore', 'thus', 'hence', 'nevertheless', 'nonetheless'
        }
        
    def calculate_perplexity_score(self, text: str) -> float:
        """
        è¨ˆç®—æ–‡å­—çš„è¤‡é›œåº¦åˆ†æ•¸
        AI æ–‡ç« é€šå¸¸æœ‰è¼ƒä½çš„è¤‡é›œåº¦ï¼ˆæ›´æµæš¢ï¼‰
        """
        words = text.lower().split()
        if len(words) < 2:
            return 0.5
        
        # è¨ˆç®—è©å½™å¤šæ¨£æ€§
        unique_ratio = len(set(words)) / len(words)
        
        # æª¢æŸ¥é‡è¤‡çš„ bigrams
        bigrams = [f"{words[i]} {words[i+1]}" for i in range(len(words)-1)]
        bigram_diversity = len(set(bigrams)) / len(bigrams) if bigrams else 0.5
        
        return (unique_ratio + bigram_diversity) / 2
    
    def check_sentence_uniformity(self, sentences: list) -> float:
        """æª¢æŸ¥å¥å­é•·åº¦çš„å‡å‹»æ€§ - AI é€šå¸¸æ›´å‡å‹»"""
        if len(sentences) < 2:
            return 0.5
        
        lengths = [len(s.split()) for s in sentences if s.strip()]
        if not lengths:
            return 0.5
        
        # è¨ˆç®—è®Šç•°ä¿‚æ•¸
        mean_len = np.mean(lengths)
        std_len = np.std(lengths)
        cv = std_len / mean_len if mean_len > 0 else 0
        
        # CV è¶Šå°è¡¨ç¤ºè¶Šå‡å‹»ï¼ˆæ›´åƒ AIï¼‰
        uniformity_score = max(0, min(1, 1 - cv))
        return uniformity_score
    
    def count_ai_markers(self, text: str) -> float:
        """è¨ˆç®— AI å¸¸ç”¨è©çš„å‡ºç¾é »ç‡"""
        words = set(text.lower().split())
        marker_count = len(words.intersection(self.ai_markers))
        return min(1.0, marker_count / 3)  # æ­£è¦åŒ–åˆ° 0-1
    
    def predict(self, text: str) -> Dict[str, float]:
        """
        ä½¿ç”¨å¤šç¨®å•Ÿç™¼å¼è¦å‰‡é æ¸¬
        """
        if not text or len(text.strip()) < 10:
            return {
                "prediction": "Unknown",
                "ai_probability": 0.5,
                "human_probability": 0.5,
                "confidence": 0
            }
        
        # è¨ˆç®—åŸºæœ¬ç‰¹å¾µ
        words = text.split()
        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]
        
        word_count = len(words)
        avg_word_length = np.mean([len(word) for word in words]) if words else 0
        avg_sentence_length = len(words) / len(sentences) if sentences else 0
        
        # å¤šç¶­åº¦è©•åˆ†
        ai_score = 0.0
        weights = []
        
        # 1. å¥å­é•·åº¦å‡å‹»æ€§ (æ¬Šé‡: 25%)
        uniformity = self.check_sentence_uniformity(sentences)
        ai_score += uniformity * 0.25
        weights.append(0.25)
        
        # 2. æ–‡å­—è¤‡é›œåº¦ (æ¬Šé‡: 20%)
        perplexity = self.calculate_perplexity_score(text)
        # AI æ–‡ç« é€šå¸¸æœ‰è¼ƒé«˜çš„è¤‡é›œåº¦åˆ†æ•¸ï¼ˆæ›´æµæš¢ï¼‰
        ai_score += perplexity * 0.20
        weights.append(0.20)
        
        # 3. AI å¸¸ç”¨è©æ¨™è¨˜ (æ¬Šé‡: 15%)
        marker_score = self.count_ai_markers(text)
        ai_score += marker_score * 0.15
        weights.append(0.15)
        
        # 4. å¹³å‡å¥å­é•·åº¦ (æ¬Šé‡: 20%)
        # AI é€šå¸¸ä¿æŒåœ¨ 15-25 å€‹è©ä¹‹é–“
        if 15 <= avg_sentence_length <= 25:
            sentence_score = 1.0
        elif 10 <= avg_sentence_length < 15 or 25 < avg_sentence_length <= 30:
            sentence_score = 0.6
        else:
            sentence_score = 0.3
        ai_score += sentence_score * 0.20
        weights.append(0.20)
        
        # 5. ç”¨è©æ­£å¼åº¦ (æ¬Šé‡: 10%)
        # AI é€šå¸¸ç”¨è¼ƒé•·çš„è©
        formality_score = min(1.0, (avg_word_length - 3) / 4) if avg_word_length > 3 else 0
        ai_score += formality_score * 0.10
        weights.append(0.10)
        
        # 6. æ–‡ç« å®Œæ•´åº¦ (æ¬Šé‡: 10%)
        # AI é€šå¸¸ç”¢ç”Ÿè¼ƒå®Œæ•´çš„æ–‡ç« 
        completeness_score = min(1.0, word_count / 100) if word_count > 50 else 0.3
        ai_score += completeness_score * 0.10
        weights.append(0.10)
        
        # æ­£è¦åŒ– AI æ©Ÿç‡
        ai_prob = min(0.95, max(0.05, ai_score))
        human_prob = 1 - ai_prob
        
        prediction = "AI" if ai_prob > 0.5 else "Human"
        confidence = abs(ai_prob - 0.5) * 200  # è½‰æ›ç‚º 0-100
        
        return {
            "prediction": prediction,
            "ai_probability": ai_prob,
            "human_probability": human_prob,
            "confidence": confidence
        }
    
    def analyze_text_features(self, text: str) -> Dict[str, any]:
        """åˆ†ææ–‡å­—ç‰¹å¾µ"""
        words = text.split()
        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]
        
        return {
            "word_count": len(words),
            "sentence_count": len(sentences),
            "avg_word_length": np.mean([len(word) for word in words]) if words else 0,
            "avg_sentence_length": len(words) / len(sentences) if sentences else 0,
            "vocabulary_diversity": len(set(words)) / len(words) if words else 0
        }

# ==================== Streamlit æ‡‰ç”¨ç¨‹å¼ ====================

@st.cache_resource
def load_model():
    """è¼‰å…¥ AI æª¢æ¸¬æ¨¡å‹"""
    detector = SimpleAIDetector()
    return detector, True

# é é¢è¨­å®š
st.set_page_config(
    page_title="AI æ–‡ç« æª¢æ¸¬å™¨",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šç¾© CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        color: #1f77b4;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.2rem;
        text-align: center;
        color: #666;
        margin-bottom: 2rem;
    }
    .result-box {
        padding: 2rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
    .ai-result {
        background-color: #ffebee;
        border-left: 5px solid #f44336;
    }
    .human-result {
        background-color: #e8f5e9;
        border-left: 5px solid #4caf50;
    }
    .metric-card {
        background-color: #f5f5f5;
        padding: 1rem;
        border-radius: 5px;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ– session state
if 'detector' not in st.session_state:
    st.session_state.detector = None
    st.session_state.model_loaded = False

st.markdown('<div class="main-header">ğŸ¤– AI æ–‡ç« æª¢æ¸¬å™¨</div>', unsafe_allow_html=True)
st.markdown('<div class="sub-header">æª¢æ¸¬æ–‡ç« æ˜¯ç”± AI é‚„æ˜¯äººé¡æ’°å¯«</div>', unsafe_allow_html=True)

# å´é‚Šæ¬„
with st.sidebar:
    st.header("â„¹ï¸ é—œæ–¼")
    st.info("""
    é€™å€‹å·¥å…·ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•ä¾†åˆ†ææ–‡ç« å…§å®¹ï¼Œ
    åˆ¤æ–·æ–‡ç« æ˜¯ç”± AI é‚„æ˜¯äººé¡æ’°å¯«ã€‚
    
    **ä½¿ç”¨æ–¹æ³•ï¼š**
    1. åœ¨æ–‡å­—æ¡†ä¸­è¼¸å…¥æˆ–è²¼ä¸Šæ–‡ç« 
    2. é»æ“Šã€Œé–‹å§‹æª¢æ¸¬ã€æŒ‰éˆ•
    3. æŸ¥çœ‹æª¢æ¸¬çµæœ
    
    **æª¢æ¸¬ç‰¹å¾µï¼š**
    - å¥å­é•·åº¦å‡å‹»æ€§
    - è©å½™å¤šæ¨£æ€§
    - ç”¨è©æ­£å¼åº¦
    - AI å¸¸ç”¨è©æ¨™è¨˜
    """)
    
    st.header("ğŸ“Š æ¨¡å‹è³‡è¨Š")
    if st.button("è¼‰å…¥æ¨¡å‹"):
        with st.spinner("æ­£åœ¨è¼‰å…¥æ¨¡å‹..."):
            st.session_state.detector, st.session_state.model_loaded = load_model()
        st.success("âœ… æª¢æ¸¬å™¨å·²å°±ç·’ï¼")
    
    if st.session_state.detector is not None:
        st.success("âœ… æ¨¡å‹å·²å°±ç·’")
    else:
        st.warning("âš ï¸ è«‹å…ˆè¼‰å…¥æ¨¡å‹")
    
    st.header("ğŸ“ ç¯„ä¾‹æ–‡ç« ")
    if st.button("è¼‰å…¥ AI æ–‡ç« ç¯„ä¾‹"):
        st.session_state.text_input = """Artificial intelligence has revolutionized numerous industries in recent years. Machine learning algorithms can now process vast amounts of data with unprecedented efficiency. These technological advancements have enabled computers to perform tasks that were once exclusively human domains. From natural language processing to image recognition, AI systems continue to demonstrate remarkable capabilities. The integration of deep learning techniques has particularly enhanced the performance of these systems."""
        st.rerun()
    
    if st.button("è¼‰å…¥äººé¡æ–‡ç« ç¯„ä¾‹"):
        st.session_state.text_input = """I remember the first time I tried to write an essay. It was tough! My thoughts were all over the place, and I couldn't figure out how to organize them. But you know what? That's totally normal. Writing is messy. Sometimes I'd write a sentence, hate it, delete it, then write it again almost the same way. That's just how it goes, right?"""
        st.rerun()

# ä¸»è¦å…§å®¹å€åŸŸ
col1, col2 = st.columns([2, 1])

with col1:
    st.header("ğŸ“ è¼¸å…¥æ–‡ç« ")
    
    # æ–‡å­—è¼¸å…¥å€
    text_input = st.text_area(
        "è«‹è¼¸å…¥æˆ–è²¼ä¸Šè¦æª¢æ¸¬çš„æ–‡ç« å…§å®¹ï¼š",
        value=st.session_state.get('text_input', ''),
        height=300,
        placeholder="åœ¨é€™è£¡è¼¸å…¥æ–‡ç« å…§å®¹...",
        key="text_input"
    )
    
    # æ–‡å­—çµ±è¨ˆ
    if text_input:
        word_count = len(text_input.split())
        char_count = len(text_input)
        st.caption(f"ğŸ“Š å­—æ•¸çµ±è¨ˆï¼š{word_count} å€‹è© | {char_count} å€‹å­—å…ƒ")
    
    # æª¢æ¸¬æŒ‰éˆ•
    detect_button = st.button("ğŸ” é–‹å§‹æª¢æ¸¬", type="primary", use_container_width=True)

with col2:
    st.header("âš™ï¸ è¨­å®š")
    
    show_details = st.checkbox("é¡¯ç¤ºè©³ç´°åˆ†æ", value=True)
    show_probabilities = st.checkbox("é¡¯ç¤ºæ©Ÿç‡åœ–è¡¨", value=True)

# è™•ç†æª¢æ¸¬
if detect_button:
    if st.session_state.detector is None:
        st.error("âŒ è«‹å…ˆåœ¨å´é‚Šæ¬„è¼‰å…¥æ¨¡å‹ï¼")
    elif not text_input or len(text_input.strip()) < 10:
        st.warning("âš ï¸ è«‹è¼¸å…¥è‡³å°‘ 10 å€‹å­—å…ƒçš„æ–‡ç« å…§å®¹")
    else:
        # é¡¯ç¤ºé€²åº¦
        with st.spinner("ğŸ” æ­£åœ¨åˆ†ææ–‡ç« ..."):
            progress_bar = st.progress(0)
            for i in range(100):
                time.sleep(0.01)
                progress_bar.progress(i + 1)
            
            # é€²è¡Œé æ¸¬
            result = st.session_state.detector.predict(text_input)
        
        st.success("âœ… åˆ†æå®Œæˆï¼")
        
        # é¡¯ç¤ºçµæœ
        st.header("ğŸ“Š æª¢æ¸¬çµæœ")
        
        # ä¸»è¦çµæœ
        result_class = "ai-result" if result['prediction'] == "AI" else "human-result"
        result_icon = "ğŸ¤–" if result['prediction'] == "AI" else "ğŸ‘¤"
        
        st.markdown(f"""
        <div class="result-box {result_class}">
            <h2 style="margin:0;">{result_icon} æª¢æ¸¬çµæœï¼š{result['prediction']}</h2>
            <h3 style="margin-top:1rem;">ä¿¡å¿ƒåˆ†æ•¸ï¼š{result['confidence']:.2f}%</h3>
        </div>
        """, unsafe_allow_html=True)
        
        # æ©Ÿç‡åœ–è¡¨
        if show_probabilities:
            st.subheader("ğŸ“ˆ æ©Ÿç‡åˆ†ä½ˆ")
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric(
                    label="ğŸ¤– AI æ’°å¯«æ©Ÿç‡",
                    value=f"{result['ai_probability']*100:.2f}%"
                )
                st.progress(result['ai_probability'])
            
            with col2:
                st.metric(
                    label="ğŸ‘¤ äººé¡æ’°å¯«æ©Ÿç‡",
                    value=f"{result['human_probability']*100:.2f}%"
                )
                st.progress(result['human_probability'])
        
        # è©³ç´°åˆ†æ
        if show_details:
            st.subheader("ğŸ“ æ–‡å­—ç‰¹å¾µåˆ†æ")
            features = st.session_state.detector.analyze_text_features(text_input)
            
            cols = st.columns(5)
            
            with cols[0]:
                st.markdown(f"""
                <div class="metric-card">
                    <h4>å­—æ•¸</h4>
                    <h2>{features['word_count']}</h2>
                </div>
                """, unsafe_allow_html=True)
            
            with cols[1]:
                st.markdown(f"""
                <div class="metric-card">
                    <h4>å¥å­æ•¸</h4>
                    <h2>{features['sentence_count']}</h2>
                </div>
                """, unsafe_allow_html=True)
            
            with cols[2]:
                st.markdown(f"""
                <div class="metric-card">
                    <h4>å¹³å‡è©é•·</h4>
                    <h2>{features['avg_word_length']:.1f}</h2>
                </div>
                """, unsafe_allow_html=True)
            
            with cols[3]:
                st.markdown(f"""
                <div class="metric-card">
                    <h4>å¹³å‡å¥é•·</h4>
                    <h2>{features['avg_sentence_length']:.1f}</h2>
                </div>
                """, unsafe_allow_html=True)
            
            with cols[4]:
                st.markdown(f"""
                <div class="metric-card">
                    <h4>è©å½™å¤šæ¨£æ€§</h4>
                    <h2>{features['vocabulary_diversity']:.2f}</h2>
                </div>
                """, unsafe_allow_html=True)
        
        # è§£é‡‹èªªæ˜
        with st.expander("â“ å¦‚ä½•è§£è®€çµæœ"):
            st.markdown("""
            - **ä¿¡å¿ƒåˆ†æ•¸**ï¼šè¡¨ç¤ºæ¨¡å‹å°é æ¸¬çµæœçš„ä¿¡å¿ƒç¨‹åº¦ï¼ˆ0-100%ï¼‰
            - **æ©Ÿç‡åˆ†ä½ˆ**ï¼šé¡¯ç¤ºæ–‡ç« å±¬æ–¼ AI æˆ–äººé¡æ’°å¯«çš„æ©Ÿç‡
            - **æ–‡å­—ç‰¹å¾µ**ï¼šåˆ†ææ–‡ç« çš„åŸºæœ¬çµ±è¨ˆè³‡è¨Š
            
            **æ³¨æ„äº‹é …ï¼š**
            - æ­¤å·¥å…·åƒ…ä¾›åƒè€ƒï¼Œä¸ä¿è­‰ 100% æº–ç¢º
            - è¼ƒçŸ­çš„æ–‡ç« å¯èƒ½å½±éŸ¿æª¢æ¸¬æº–ç¢ºåº¦
            - å»ºè­°çµåˆå¤šç¨®æ–¹æ³•é€²è¡Œåˆ¤æ–·
            """)

# é å°¾
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666;">
    <p>ğŸ“ 5114056002_HW5 | Built with Streamlit & Transformers</p>
</div>
""", unsafe_allow_html=True)
